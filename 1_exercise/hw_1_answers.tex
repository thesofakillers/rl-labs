% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=1in,left=1.5in,right=1.5in,bottom=1.5in]{geometry}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
% \setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\pagenumbering{gobble}
\usepackage{setspace}
\makeatletter
\@ifpackageloaded{subfig}{}{\usepackage{subfig}}
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\captionsetup[subfloat]{margin=0.5em}
\AtBeginDocument{%
\renewcommand*\figurename{Figure}
\renewcommand*\tablename{Table}
}
\AtBeginDocument{%
\renewcommand*\listfigurename{List of Figures}
\renewcommand*\listtablename{List of Tables}
}
\newcounter{pandoccrossref@subfigures@footnote@counter}
\newenvironment{pandoccrossrefsubfigures}{%
\setcounter{pandoccrossref@subfigures@footnote@counter}{0}
\begin{figure}\centering%
\gdef\global@pandoccrossref@subfigures@footnotes{}%
\DeclareRobustCommand{\footnote}[1]{\footnotemark%
\stepcounter{pandoccrossref@subfigures@footnote@counter}%
\ifx\global@pandoccrossref@subfigures@footnotes\empty%
\gdef\global@pandoccrossref@subfigures@footnotes{{##1}}%
\else%
\g@addto@macro\global@pandoccrossref@subfigures@footnotes{, {##1}}%
\fi}}%
{\end{figure}%
\addtocounter{footnote}{-\value{pandoccrossref@subfigures@footnote@counter}}
\@for\f:=\global@pandoccrossref@subfigures@footnotes\do{\stepcounter{footnote}\footnotetext{\f}}%
\gdef\global@pandoccrossref@subfigures@footnotes{}}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Exercise Set 1 - Reinforcement Learning},
  pdfauthor={Giulio Starace - 13010840},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Exercise Set 1 - Reinforcement Learning}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{MDPs and Dynamic Programming}
\author{Giulio Starace - 13010840}
\date{\today}

\begin{document}
\maketitle

\hypertarget{homework-coding-assignment---dynamic-programming}{%
\section*{Homework: Coding Assignment - Dynamic
Programming}\label{homework-coding-assignment---dynamic-programming}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Coding answers have been submitted on codegra under the group
  ``stalwart cocky sawfly''.
\item
  Comparing policy iteration and value iteration algorithms:

  \begin{itemize}
  \item
    I expect a single iteration of value iteration to run faster than a single iteration of policy
    iteration. This is because an iteration of policy iteration involves policy evaluation, which
    itself is an iterative process requiring several sweeps across the set of states. On the other
    hand, an iteration in value iteration ``truncates'' policy evaluation by stopping after a single
    sweep.
  \item
    I expect policy iteration to take fewer total iterations than value iteration. This is because
    value iteration relies on the convergence to an optimal \textit{value function}, despite
    ultimately being interested in the optimal \textit{policy}. Since the convergence criterion is
    set to some arbitrarily small $\Delta$, and we only extract the optimal policy from the optimal
    value function after convergence, it can occur that the optimal policy is found before
    convergence. In policy iteration on the other hand, since we improve our policy at each
    iteration, we obtain a new policy at each iteration, which we can evaluate in the next iteration
    before checking for stability (and hence convergence). This potentially allows for more directed
    improvements since we are directly focusing on the policy rather than the value function.
    Furthermore this is a more frequent way of checking for \textit{policy} optimality by directly
    evaluating policies. The more frequent and directness of the convergence check intuitively
    suggests faster convergence than value iteration.
  \end{itemize} 
\end{enumerate}

\hypertarget{homework-dynamic-programming}{%
\section*{Homework: Dynamic
Programming}\label{homework-dynamic-programming}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The value \(v^\pi(s)\) of a state \(s\) under policy \(\pi\) is simply
  the expected action-value function at \(s\) over all actions \(a\)
  under \(pi\). It can hence be expressed in terms of \(\pi\) and
  \(q^\pi(s, a)\) as:

  \begin{align}
     v^\pi(s) &= \mathbb{E}_\pi\left[q^\pi(S_t, A_t) | S_t = s, A_t=a\right]\notag\\ 
     v^\pi(s) &= \sum_a \pi(s|a) q^\pi(s, a) \label{eq:sv_sav}
  \end{align}

  in the stochastic case and

  \begin{equation}
     v^\pi(s) =  q^\pi(s, \pi(s))
  \end{equation}

  in the deterministic case.

\item
  The value iteration update can be re-written in terms of Q-values as such:
  
  \begin{equation}
    q_{k+1}(s, a) = \sum_{s', r} p(s', r | s, a)\left[r + \gamma \max_{a'} q_k(s', a')\right],
  \end{equation}
  obtaining the Q-value iteration update. This is done by turning the Bellman optimality equation
  for $q_*(s, a)$ into an update rule, as is done for the value iteration update.

\item
  A new policy evaluation update in terms of $Q^\pi(s, a)$ instead of $V^\pi(s)$ can be obtained by
  writing the following, making use of equation (\ref{eq:sv_sav}):
  \begin{equation}
    Q^\pi(s, a) \leftarrow \sum_{s', r} p(s', r| s, a)\left[r + \gamma \sum_{a'}\pi(a|s)Q^\pi(s', a')\right]
  \end{equation}
\item

  The policy improvement step of the policy iteration algorithm can be rewritten in terms of
  $Q^\pi(s, a)$ instead of $V^\pi(s)$ as follows:

  \begin{equation}
    \pi(s) \leftarrow \arg\max_a Q^{\pi'}(s, a),
  \end{equation}

  where $\pi'$ is the old policy we are improving.
  
\item
  \par On page 75, policy evaluation involves a weighted summation over actions stochastically outputted
  by the policy multiplied by the sum over transition-weighted state rewards and state-values. On
  page 80, the summation over actions is not present, and we compute the action in the probability
  term deterministically by using the policy previously computed in the policy improvement step. 
  \par We can make this distinction precisely because in one case we apply the policy stochastically
  (therefore requiring an expectation over actions, and hence summation) while in the other case we
  apply it deterministically. The usage of the policy in the policy evaluation step of policy
  iteration is deterministic because we have fixed the policy after greedily obtaining it via policy
  improvement and are now evaluating this policy specifically.

\end{enumerate}

\end{document}
