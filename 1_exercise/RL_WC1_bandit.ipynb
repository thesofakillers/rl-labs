{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8760cdd25838cc879d075523dab9826c",
     "grade": false,
     "grade_id": "cell-cf757d55465d34ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "41a4867ada0790944d003892e913f8ac",
     "grade": false,
     "grade_id": "cell-33c0d426515d5c1b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## The Multi-armed bandit problem\n",
    "Imagine you're faced with a number of slot machines (also called 'bandits'), each with a lever ('arm') to pull. Upon pulling a particular lever, it will give a random reward from an unknown distribution particular to that lever. The goal of the multi-armed bandit (MAB) problem, is to maximise your total reward given that you are allowed to pull levers a fixed number of times total (this is called your 'budget').\n",
    "\n",
    "A basic strategy might be to spend some of your budget pulling different levers to get an idea of which levers give the most reward ('exploration'). After this, you may choose increasingly often pull the lever that you expect gives the most reward ('exploitation'). The question then is: how much exploration and how much exploitation makes the optimal strategy? This 'exploration-exploitation trade-off' is a classic feature of reinforcement learning problems: we have to interact with the environment to gather data, and we must choose an optimal way of interacting on the fly. \n",
    "\n",
    "This notebook provides a MAB environment to interact with. Spend some time pulling levers to get a feeling for the problem (see cells below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2363716f89f42b739db88c2169e91792",
     "grade": false,
     "grade_id": "cell-44280a71d447cc9b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A bandit gives a random reward from a particular Gaussian distribution.\n",
    "class Bandit:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def sample(self):\n",
    "        return np.random.normal(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    def __init__(self, num_arms=10, means=None, stds=None):\n",
    "        if means is None:\n",
    "            self.means = np.random.uniform(0, 5, num_arms)\n",
    "        else:\n",
    "            self.means = means\n",
    "        if stds is None:\n",
    "            self.stds = np.random.uniform(0, 3, num_arms)\n",
    "        else:\n",
    "            self.stds = stds \n",
    "        self.bandits = [Bandit(mean, std) for mean, std in zip(self.means, self.stds)]\n",
    "        self.arms_pulled = np.zeros(num_arms, dtype=int)\n",
    "        self.arms_rewards = np.zeros(num_arms)\n",
    "        self.num_arms = num_arms\n",
    "        \n",
    "    def reset(self):\n",
    "        self.__init__(self.num_arms, self.means, self.stds)\n",
    "        \n",
    "    def sample(self, i):\n",
    "        reward = self.bandits[i].sample()\n",
    "        self.arms_pulled[i] += 1\n",
    "        self.arms_rewards[i] += reward\n",
    "        return reward\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.arms_rewards, self.arms_pulled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "23b49e7e061436bf569e3b311dbb553c",
     "grade": false,
     "grade_id": "cell-988e898278d87b8f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Get a feeling\n",
    "Play around with the arms for a minute by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example interaction \n",
    "num_arms = 4\n",
    "mab = MultiArmedBandit(num_arms)\n",
    "for _ in range(10):\n",
    "    arm = int(input(f\"Choose an arm to pull [0-{num_arms-1}]: \"))\n",
    "    assert arm >=0 and arm < num_arms, f\"Arm must be an integer in the interval [0, {num_arms - 1}] inclusive.\"\n",
    "    print(\" Reward: {:.3f}\".format(mab.sample(arm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d52232788b578d130afb0a31a3a72d11",
     "grade": false,
     "grade_id": "cell-85d2324c09175d8e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Example estimation\n",
    "Below is an example interaction that tries to estimate the best arm of a 10-armed bandit from 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example interaction\n",
    "num_arms = 10\n",
    "mab = MultiArmedBandit(num_arms)\n",
    "# Sample 100 random arms\n",
    "for _ in range(100):\n",
    "    action = np.random.choice(num_arms)\n",
    "    reward = mab.sample(action)\n",
    "\n",
    "# Get how many times arms were pulled and how much total reward was generated by those arms.\n",
    "# Together these arrays represent the state of the MAB.\n",
    "state = mab.get_state()\n",
    "arms_rewards, arms_pulled = state\n",
    "# Get average reward per arm\n",
    "arms_average_reward = arms_rewards / arms_pulled\n",
    "\n",
    "# Inspect results\n",
    "best_arm, best_reward = -1, -10e3\n",
    "for i, average_reward in enumerate(arms_average_reward):\n",
    "    print('Arm {} yielded average reward: {:.3f}'.format(i, average_reward))\n",
    "    if average_reward > best_reward:\n",
    "        best_reward = average_reward\n",
    "        best_arm = i\n",
    "print('\\nWe think the best arm is arm {}'.format(best_arm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a09a92c5de85bcf0fa44b4c4575e700b",
     "grade": false,
     "grade_id": "cell-ef68c404eea507e6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The goal of this exercise is to get a feeling for this MAB problem. In order to do this, you're tasked with writing a strategy (policy) that maximises the expected reward, or - equivalently - mimises the expected regret (where the expectation is taken over multiple simulations where a new MAB is instantiated each time). Regret of a policy is defined here as: expected optimal reward - expected obtained reward. That is: it is the difference between how much reward an oracle that knows the optimal lever to pull would have obtained, and the reward the implemented policy obtains.\n",
    "\n",
    "Below a 'simulate_policies' function is provided that calculates this expected regret, given a policy (or list of policies for fair comparison of policies). A policy is a function that takes as input a state (in this case the tuple (arms_pulled, arms_rewards)), and outputs an action (in this case an integer in the interval [0, num_arms - 1] inclusive). Two example policies are provided: random, and a policy that starts random (we call this a burn-in period) and then proceeds to pull the lever it thinks is best based on the statistics gathered during the burn-in. \n",
    "\n",
    "This last policy is a very naive way to dealing with the exploration-exploitation trade-off: first we explore for a set number of samples, then we exploit for the rest of the budget. See if you can write a policy that improves over this one. Note that to really evaluate this well we need to run simulations very often, which might be infeasible given the time we want to spend on this notebook. If you get something that does approximately as well as this policy, consider it a success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc25ed0fe0ef4c1a71b958f48bcd4494",
     "grade": false,
     "grade_id": "cell-83ca914fa89aaca8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def episode(policy, budget, mab=None, num_arms=10):\n",
    "    \"\"\"\n",
    "    Function used to simulate an episode. Takes as input a policy, and outputs regret.\n",
    "    \n",
    "    Args:\n",
    "        policy (callable): A function that takes as input a state tuple (arms_rewards, arms_pulled)\n",
    "            and outputs an integer in the interval [0, num_arms - 1] inclusive that represents the\n",
    "            action to take.\n",
    "        budget: number of samples to draw before an episode terminates. \n",
    "        \n",
    "    Returns:\n",
    "        average_regret (float): average regret over the episode.\n",
    "    \"\"\"\n",
    "    if mab is None:\n",
    "        mab = MultiArmedBandit(num_arms)\n",
    "    optimal_reward = np.max(mab.means) * budget\n",
    "    for _ in range(budget):\n",
    "        state = mab.get_state()\n",
    "        choice = policy(state)\n",
    "        mab.sample(choice)\n",
    "    total_reward = np.sum(mab.arms_rewards)\n",
    "    regret = (optimal_reward - total_reward)\n",
    "    return regret\n",
    "\n",
    "\n",
    "def simulate_policies(policies, num_arms=10, budget=1000, num_simulations=100):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        policies (callable or list of callables): A list of functions that each take as input a state \n",
    "            tuple (arms_rewards, arms_pulled) and output an integer in the interval [0, num_arms - 1] \n",
    "            inclusive that represents the action to take.\n",
    "        num_arms: number of arms on the MultiArmedBandit.\n",
    "        budget: number of samples to draw before an episode terminates.\n",
    "        num_simulations: number of episodes to average the results over.   \n",
    "        \n",
    "    Returns:\n",
    "        expected_regrets (list or float): list of expected regrets corresponding to the policies. Float\n",
    "            if a single policy was evaluated.\n",
    "    \"\"\"\n",
    "    if not isinstance(policies, list):\n",
    "        policies = [policies]\n",
    "    average_regrets = np.zeros(len(policies))\n",
    "    for _ in range(num_simulations):\n",
    "        mab = MultiArmedBandit(num_arms)\n",
    "        for i, policy in enumerate(policies):\n",
    "            if i > 0:\n",
    "                mab.reset()\n",
    "            regret = episode(policy, budget, mab)\n",
    "            average_regrets[i] += regret / num_simulations\n",
    "            \n",
    "    if len(average_regrets) == 1:\n",
    "        return average_regrets[0]\n",
    "    return list(average_regrets)\n",
    "\n",
    "\n",
    "def random_policy(state):\n",
    "    \"\"\"\n",
    "    Random policy.\n",
    "    \n",
    "    Args:\n",
    "        state (tuple): a tuple (arms_rewards, arms_pulled) representing the state of the MAB.\n",
    "            'arms_rewards' is a numpy array of length num_arms, that represents the total reward obtained\n",
    "            from pulling arms. 'arms_pulled' is a numpy array of the same length that represents the \n",
    "            number of times a particular arm was pulled.\n",
    "            \n",
    "    Returns:\n",
    "        action (int): integer in the interval [0, num_arms - 1] inclusive, representing the arm to pull.\n",
    "    \"\"\"\n",
    "    arms_rewards, arms_pulled = state\n",
    "    action = np.random.choice(len(arms_rewards))\n",
    "    return action\n",
    "\n",
    "\n",
    "def max_policy_with_burnin(state, burnin=100):\n",
    "    \"\"\"\n",
    "    Policy that selects random levers during a burn-in (exploration), followed by \n",
    "    exploitation of the optimal lever according to the gathered statistics.\n",
    "\n",
    "    Args:\n",
    "        state (tuple): a tuple (arms_rewards, arms_pulled) representing the state of the MAB.\n",
    "            'arms_rewards' is a numpy array of length num_arms, that represents the total reward obtained\n",
    "            from pulling arms. 'arms_pulled' is a numpy array of the same length that represents the \n",
    "            number of times a particular arm was pulled.\n",
    "            \n",
    "    Returns:\n",
    "        action (int): integer in the interval [0, num_arms - 1] inclusive, representing the arm to pull.\n",
    "    \"\"\"\n",
    "    arms_rewards, arms_pulled = state\n",
    "    if np.sum(arms_pulled) < burnin:\n",
    "        action = np.random.choice(len(arms_rewards))\n",
    "        return action\n",
    "    average_arm_reward = arms_rewards / arms_pulled\n",
    "    action = np.argmax(average_arm_reward)\n",
    "    return action\n",
    "\n",
    "policies = [random_policy, max_policy_with_burnin]\n",
    "random_policy_regret, max_policy_regret = simulate_policies(policies)\n",
    "print('Random policy regret: {:.2f}'.format(random_policy_regret))\n",
    "print('Max policy regret: {:.2f}'.format(max_policy_regret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2c91d82b9cb65d630061ed020afebe70",
     "grade": true,
     "grade_id": "cell-02b174a39f0c36fd",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def my_policy(state):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        state (tuple): a tuple (arms_rewards, arms_pulled) representing the state of the MAB.\n",
    "            'arms_rewards' is a numpy array of length num_arms, that represents the total reward obtained\n",
    "            from pulling arms. 'arms_pulled' is a numpy array of the same length that represents the \n",
    "            number of times a particular arm was pulled.\n",
    "            \n",
    "    Returns:\n",
    "        action (int): integer in the interval [0, num_arms - 1] inclusive, representing the arm to pull.\n",
    "    \"\"\"\n",
    "    \n",
    "    arms_rewards, arms_pulled = state\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return action\n",
    "\n",
    "policies = [my_policy, max_policy_with_burnin]\n",
    "my_policy_regret, max_policy_regret = simulate_policies(policies)\n",
    "print('My policy regret: {:.2f}'.format(my_policy_regret))\n",
    "print('Max policy regret: {:.2f}'.format(max_policy_regret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
