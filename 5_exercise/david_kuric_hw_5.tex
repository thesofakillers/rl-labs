\documentclass{article}

\usepackage[top=1in,left=1.5in,right=1.5in,bottom=1.5in]{geometry}

\usepackage{pgfplots}
\usetikzlibrary{angles, arrows.meta, calc, quotes}
\pgfplotsset{width=0.8\textwidth,compat=1.18}

\usepackage{subcaption}

\usepackage{booktabs}

\usepackage{graphicx}
\let\rfb\reflectbox
\graphicspath{ {images} }

\usepackage{cancel}

\usepackage{mathtools}

\usepackage{nicefrac}
\newcommand{\flippedfrac}[2]{\rfb{\nicefrac{\rfb{#2}}{\rfb{#1}}}}



\usepackage{amsthm}
\renewcommand{\qedsymbol}{$\blacksquare$}
\usepackage{amssymb}
\usepackage{amsmath}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
}


\usepackage{titling}
\title{Exercise Set 5 - Reinforcement Learning}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}
\makeatother
\subtitle{Advanced policy-based methods}
\author{Giulio Starace - 13010840}
\date{\today}

\begin{document}
\maketitle
\setcounter{section}{9}
\setcounter{subsection}{3}
\subsection{Homework: Limits of policy gradients}
\begin{enumerate}
	\item Given our policy:
	      \begin{equation}
		      \pi(a|s, \theta)=\frac{1}{\sigma\left(\theta_\sigma\right) \sqrt{2 \pi}} \exp
		      \left(-\frac{\left(a-\mu\left(\theta_\mu\right)\right)^2}{2
				      \sigma\left(\theta_\sigma\right)^2}\right),
	      \end{equation}
	      We can compute $\nabla \log \pi(a|s, \theta)$ w.r.t. $\theta_\mu$ and $\theta_\sigma$ by
	      applying the chain rule. Let $\log \pi(a|s, \theta)$ be $L(\theta)$, then w.r.t. a given
	      param $\theta_i$ we have:
	      \begin{align*}
		      \nabla_{\theta_i} \log \pi(a|s, \theta) & = \nabla_{\theta_i} L(\theta)                   \\
		                                              & = \frac{\partial L(\theta)}{\partial \pi} \cdot
		      \frac{\partial \pi(a|s,\theta)}{\partial \theta_i}.
	      \end{align*}
	      When $\theta_i = \theta_\mu$, we have:
	      \begin{align*}
		      \nabla_{\theta_\mu} \log \pi(a|s, \theta) & = \frac{\partial L(\theta)}{\partial \pi,
		      } \cdot \frac{\partial \pi(a|s,\theta)}{\partial \theta_\mu}                               \\
		                                                & =  \frac{\partial L(\theta)}{\partial \pi(a|s,
			      \theta)} \cdot \frac{\partial \pi(a|s,\theta)}{\partial \mu} \cdot
		      \frac{\partial{\mu(\theta_\mu)}}{\partial \theta_\mu}. \numberthis \label{eq:grad_mu}
	      \end{align*}
	      When $\theta_i = \theta_\sigma$, we have:
	      \begin{align*}
		      \nabla_{\theta_\sigma} \log \pi(a|s, \theta) & = \frac{\partial L(\theta)}{\partial \pi,
		      } \cdot \frac{\partial \pi(a|s,\theta)}{\partial \theta_\sigma}                               \\
		                                                   & =  \frac{\partial L(\theta)}{\partial \pi(a|s,
			      \theta)} \cdot \frac{\partial \pi(a|s,\theta)}{\partial \sigma} \cdot
		      \frac{\partial{\sigma(\theta_\sigma)}}{\partial \theta_\sigma}. \numberthis \label{eq:grad_sigma}
	      \end{align*}
	      The first two terms of equations (\ref{eq:grad_mu}) and (\ref{eq:grad_sigma}) will be the
	      same regardless of parametrization. We get
	      \begin{align}
		      \frac{\partial L(\theta)}{\partial \theta} & = \frac{1}{\pi(a|s,\theta)} \label{eq:dl_dpi},
		      \\ \frac{\partial \pi(a|s,\theta)}{\partial \mu}    & = \frac{a-\mu(\theta_\mu)}{\sqrt{2
				      \pi } \sigma(\theta_\sigma) ^3} \exp\left[-\frac{(a-\mu(\theta_\mu))^2}{2
				      \sigma(\theta_\sigma) ^2}\right] = \frac{a
			      - \mu(\theta_\mu)}{\sigma(\theta_\sigma)^2} \pi(a|s,\theta), \label{eq:dpi_dmu}
		      \\ \frac{\partial \pi(a|s,\theta)}{\partial \sigma} & = \frac{
			      (a-\mu(\theta_\mu))^2-\sigma(\theta_\sigma) ^2}{\sqrt{2 \pi } \sigma(\theta_\sigma) ^4}
		      \exp\left[-\frac{(a-\mu(\theta_\mu) )^2}{2 \sigma(\theta_\sigma) ^2}\right] = \frac{
			      (a-\mu(\theta_\mu))^2-\sigma(\theta_\sigma) ^2}{\sigma(\theta_\sigma)^3}
		      \pi(a|s,\theta).\label{eq:dpi_dsigma}
	      \end{align}
	      Equations (\ref{eq:grad_mu}) and (\ref{eq:grad_sigma}) can be then further simplified as:
	      \begin{align}
		      \nabla_{\theta_\mu} \log \pi(a|s, \theta)    & = \frac{a
			      - \mu(\theta_\mu)}{\sigma(\theta_\sigma)^2} \cdot
		      \frac{\partial{\mu(\theta_\mu)}}{\partial \theta_\mu}. \label{eq:grad_mu_simple} \\
		      \nabla_{\theta_\sigma} \log \pi(a|s, \theta) & = \frac{
			      (a-\mu(\theta_\mu))^2-\sigma(\theta_\sigma) ^2}{\sigma(\theta_\sigma)^3}\cdot
		      \frac{\partial{\sigma(\theta_\sigma)}}{\partial \theta_\sigma}.
		      \label{eq:grad_sigma_simple}
	      \end{align}
	      We are then left with determining the final terms of equations (\ref{eq:grad_mu_simple}) and
	      (\ref{eq:grad_sigma_simple}) for different parametrizations.
	      \begin{enumerate}
		      \item When $\mu(\theta_\mu) = \theta_\mu$ and $\sigma(\theta_\sigma) = \exp(\theta_\sigma)$, we
		            get
		            \begin{align}
			            \frac{\partial \mu(\theta_\mu)}{\partial \theta_\mu} & = 1,
			            \\ \frac{\partial{\sigma(\theta_\sigma)}}{\partial \theta_\sigma}
			                                                                 & = \exp(\theta_\sigma).
		            \end{align}
		            We can plug this into equations (\ref{eq:grad_mu_simple}) and
		            (\ref{eq:grad_sigma_simple}) along with the updated terms from previously and get:
		            \begin{align}
			            \nabla_{\theta_\mu} \log \pi(a|s, \theta)
			                                                         & = \frac{a-\theta_\mu}{\exp^2(\theta_\sigma)} \label{eq:grad_mu_first} \\
			            \nabla_{\theta_\sigma} \log \pi(a|s, \theta) & = \frac{
				            (a-\theta_\mu)^2-\exp^2(\theta_\sigma) }{\exp^2(\theta_\sigma)}
			            \label{eq:grad_sigma_first}.
		            \end{align}
		      \item When $\mu(\theta_\mu) = \theta_\mu$ and $\sigma(\theta_\sigma) = \theta_\sigma^2$,
		            we get
		            \begin{align}
			            \frac{\partial \mu(\theta_\mu)}{\partial \theta_\mu} & = 1,
			            \\ \frac{\partial{\sigma(\theta_\sigma)}}{\partial \theta_\sigma}
			                                                                 & = 2 \theta_\sigma.
		            \end{align}
		            We can once again plug this into equations (\ref{eq:grad_mu_simple}) and
		            (\ref{eq:grad_sigma_simple}) along with the updated terms from previously and get:
		            \begin{align}
			            \nabla_{\theta_\mu} \log \pi(a|s, \theta)
			                                                         & = \frac{a-\theta_\mu}{\theta_\sigma^4}
			            \label{eq:grad_mu_second}                                                             \\
			            \nabla_{\theta_\sigma} \log \pi(a|s, \theta) & = 2 \cdot
			            \frac{(a-\theta_\mu)^2-\theta_\sigma^4}{\theta_\sigma^5} \label{eq:grad_sigma_second}.
		            \end{align}
	      \end{enumerate}
	\item The policy gradient update for a given parameter $\theta_i$ can be computed with
	      \begin{equation}
		      \theta_i'=  \theta_i + \alpha \cdot r \cdot \nabla_{\theta_i} \log \pi(a_t|s_t, \theta),
	      \end{equation}
	      where $r$ is the reward and $\alpha$ is the learning rate. Given $r = 3$ and $\alpha = 0.1$,
	      we write
	      \begin{equation}
		      \theta_i'=  \theta_i + 0.3 \cdot \nabla_{\theta_i} \log \pi(a_t|s_t, \theta).
	      \end{equation}
	      \begin{enumerate}
		      \item When $\mu(\theta_\mu) = \theta_\mu = 0$ and $\sigma(\theta_\sigma)
			            = \exp(\theta_\sigma) = 4$, we get that $\theta_\mu = 0$ and $\theta_\sigma
			            = \log(4)$. We can plug these values into equations (\ref{eq:grad_mu_first}) and
		            (\ref{eq:grad_sigma_first}), along with the given $a=3$ and get:
		            \begin{align*}
			            \nabla_{\theta_\mu} \log \pi(a|s, \theta)    & = \frac{3-0}{\exp^2(\log(4))}
			            = \frac{3}{16} = 0.1875,                                                     \\
			            \nabla_{\theta_\sigma} \log \pi(a|s, \theta) & = \frac{
				            (3-0)^2-\exp^2(\log(4)) }{\exp^2(\log(4)))} = \frac{9 - 16}{16} = -0.4375.
		            \end{align*}
		            We can finish plugging in values for the update and get:
		            \begin{align}
			            \theta_\mu'    & =  0 + 0.3 \cdot 0.1875 = 0.05625,             \\
			            \theta_\sigma' & =  \log(4) - 0.3 \cdot 0.4375 = 1.2550443611.
		            \end{align}
		            The new policy $\mathcal{N}(\sigma(\theta_\mu'), \sigma(\theta_\sigma'))$ is
		            \begin{equation}
			            \pi(a|s, \theta) = \frac{1}{1.2550443611 \cdot \sqrt{2 \pi}}\exp\left[-\frac{(3
					            - 0.05625)^2}{2 \cdot 1.2550443611^2}\right]
		            \end{equation}
		      \item When $\mu(\theta_\mu) = \theta_\mu = 0$ and $\sigma(\theta_\sigma) = \theta_\sigma^2
			            = 4$, we get that $\theta_\mu = 0$ and $\theta_\sigma = \pm ~ 2$. We can plug these
		            values into equations (\ref{eq:grad_mu_second}) and (\ref{eq:grad_sigma_second}), along
		            with the given $a=3$ and get:
		            \begin{align*}
			            \nabla_{\theta_\mu} \log \pi(a|s, \theta)    & = \frac{3-0}{(\pm~2)^4}
			            = \frac{3}{16} = 0.1875,                                               \\
			            \nabla_{\theta_\sigma} \log \pi(a|s, \theta) & = 2 \cdot \frac{
				            (3-0)^2-(\pm~2)^4 }{(\pm ~2)^5} = \pm 2 \cdot \frac{9 - 16}{32} = \mp 0.4375
		            \end{align*}
		            We can finish plugging in values for the update and get:
		            \begin{align}
			            \theta_\mu'    & =  0 + 0.3 \cdot 0.1875 = 0.05625,               \\
			            \theta_\sigma' & =  \pm~ 2 \mp~ 0.3 \cdot 0.4375 = \pm~ 1.86875.
		            \end{align}
		            The new policy $\mathcal{N}(\sigma(\theta_\mu'), \sigma(\theta_\sigma'))$ is
		            \begin{equation}
			            \pi(a|s, \theta) = \pm~ \frac{1}{1.86875 \cdot \sqrt{2 \pi}}\exp\left[-\frac{(3
					            - 0.05625)^2}{2 \cdot 1.86875^2}\right]
		            \end{equation}
	      \end{enumerate}
	\item As can be noted from our results, a drawback to policy gradients is that different
	      parametrizations will lead to the same gradients.
\end{enumerate}

PS: I can't help but comment that parts 1 and 2 of this question were extremely tedious to answer,
and amounted mostly to busy work (very little RL knowledge was being assessed here), particularly
factoring in the \LaTeX~ typesetting. Furthermore, because of the tediousness in parts 1 and 2, they
are very prone to error, potentially leading to erroneous results which could negatively influence
the discussion in 3 (mistake propagation). Please consider this feedback for future assignments.
\subsection{Homework: Coding Assignment - Policy Gradients}
\begin{enumerate}
	\item Two advantages of using policy based methods over value based methods are:
	      \begin{enumerate}
		      \item Policy based methods can more easily be applied to problems with large and/or
		            continuous state spaces. Unlike value based methods, policy based methods do not
		            need to compute the value of each state, and furthermore do not need to find a way
		            to compute a maximum over all possible state values, which can be prohibitively
		            expensive. Policy based methods circumvent this issue by directly adjusting the
		            parameters of the policy function, directly estimating the action probability
		            distribution for a given input state.
		      \item Policy based methods provide a natural way of learning stochastic policies. In value
		            based methods, stochasticity is typically manually governed by the $\epsilon$
		            hyperparameter used in $epsilon$-greedy policies. In policy based methods, the optimal
		            stochasticity with arbitrary action probabilities can be learned.
	      \end{enumerate}
	\item Coding answers have been submitted on codegra under the group ``stalwart cocky sawly".
\end{enumerate}

\setcounter{section}{10}
\setcounter{subsection}{2}
\subsection{Homework: Update Directions}
\begin{enumerate}
	\item hello world
	\item hello world
	\item hello world
\end{enumerate}

\end{document}

